package provideroptimizer

import (
	"strconv"
	"testing"
	"time"

	"cosmossdk.io/math"
	"github.com/lavanet/lava/v5/utils"
	"github.com/lavanet/lava/v5/utils/lavaslices"
	"github.com/lavanet/lava/v5/utils/rand"
	spectypes "github.com/lavanet/lava/v5/x/spec/types"
	"github.com/stretchr/testify/require"
)

const (
	TEST_AVERAGE_BLOCK_TIME = 10 * time.Second
	TEST_BASE_WORLD_LATENCY = 10 * time.Millisecond // same as score.DefaultLatencyNum
)

func setupProviderOptimizer(maxProvidersCount uint) *ProviderOptimizer {
	averageBlockTIme := TEST_AVERAGE_BLOCK_TIME
	return NewProviderOptimizer(StrategyBalanced, averageBlockTIme, maxProvidersCount, nil, "test")
}

type providersGenerator struct {
	providersAddresses []string
}

func (pg *providersGenerator) setupProvidersForTest(count int) *providersGenerator {
	pg.providersAddresses = make([]string, count)
	for i := range pg.providersAddresses {
		pg.providersAddresses[i] = "lava@test_" + strconv.Itoa(i)
	}
	return pg
}

// TestProviderOptimizerProviderDataSetGet tests that the providerData
// Get and Set methods work as expected
func TestProviderOptimizerProviderDataSetGet(t *testing.T) {
	providerOptimizer := setupProviderOptimizer(1)
	providersGen := (&providersGenerator{}).setupProvidersForTest(1)
	providerAddress := providersGen.providersAddresses[0]
	for i := 0; i < 100; i++ {
		providerData := ProviderData{SyncBlock: uint64(i)}
		address := providerAddress + strconv.Itoa(i)
		set := providerOptimizer.providersStorage.Set(address, providerData, 1)
		if set == false {
			utils.LavaFormatWarning("set in cache dropped", nil)
		}
	}
	time.Sleep(4 * time.Millisecond)
	for i := 0; i < 100; i++ {
		address := providerAddress + strconv.Itoa(i)
		providerData, found := providerOptimizer.getProviderData(address)
		require.Equal(t, uint64(i), providerData.SyncBlock, "failed getting entry %s", address)
		require.True(t, found)
	}
}

// TestProviderOptimizerBasicProbeData tests the basic provider optimizer operation
// when it is updated with probe relays. Providers with bad scores should have a worse chance
// to be picked (and vice versa).
// Scenario:
//  0. There are 10 providers, the optimizer is configured to pick a single provider
//  1. Choose between 10 identical providers -> none should be in the worst tier
//  2. Append bad probe relay data for providers 5-7 and pick providers -> should not be 6-8
//  3. Append good probe relay data for providers 0-2 and pick providers -> should often be 0-2
func TestProviderOptimizerBasicProbeData(t *testing.T) {
	providerOptimizer := setupProviderOptimizer(1)
	providersGen := (&providersGenerator{}).setupProvidersForTest(10)
	rand.InitRandomSeed()
	cu := uint64(10)
	requestBlock := int64(1000)

	// damage providers 5-7 scores with bad latency probes relays
	// they should not be selected by the optimizer and should be in the worst tier
	badLatency := TEST_BASE_WORLD_LATENCY * 3
	providerOptimizer.AppendProbeRelayData(providersGen.providersAddresses[5], badLatency, true)
	providerOptimizer.AppendProbeRelayData(providersGen.providersAddresses[6], badLatency, true)
	providerOptimizer.AppendProbeRelayData(providersGen.providersAddresses[7], badLatency, true)
	time.Sleep(4 * time.Millisecond)
	// returnedProviders, tier = providerOptimizer.ChooseProvider(providersGen.providersAddresses, nil, cu, requestBlock)

	// improve providers 0-2 scores with good latency probes relays
	// they should be selected by the optimizer more often
	goodLatency := TEST_BASE_WORLD_LATENCY / 2
	providerOptimizer.AppendProbeRelayData(providersGen.providersAddresses[0], goodLatency, true)
	providerOptimizer.AppendProbeRelayData(providersGen.providersAddresses[1], goodLatency, true)
	providerOptimizer.AppendProbeRelayData(providersGen.providersAddresses[2], goodLatency, true)
	time.Sleep(4 * time.Millisecond)
	results, _ := runChooseManyTimesAndReturnResults(t, providerOptimizer, providersGen.providersAddresses, nil, 1000, cu, requestBlock)

	// With weighted selection, good latency providers should collectively get more selections
	goodProviderSelections := results[providersGen.providersAddresses[0]] + results[providersGen.providersAddresses[1]] + results[providersGen.providersAddresses[2]]
	require.Greater(t, goodProviderSelections, 250, results, "good latency providers should collectively get >25% of selections")
}

// runChooseManyTimesAndReturnResults uses the given optimizer and providers addresses
// to pick providers <times> times and return two results maps:
//  1. results: map of provider address to the number of times it was picked
//  2. tierResults: map of tier and the number of times a provider from the specific tier was picked
func runChooseManyTimesAndReturnResults(t *testing.T, providerOptimizer *ProviderOptimizer, providers []string, ignoredProviders map[string]struct{}, times int, cu uint64, requestBlock int64) (map[string]int, map[int]int) {
	tierResults := make(map[int]int) // Kept for API compatibility, always returns -1 for weighted selection
	results := make(map[string]int)
	for i := 0; i < times; i++ {
		returnedProviders, tier := providerOptimizer.ChooseProvider(providers, ignoredProviders, cu, requestBlock)
		require.Equal(t, 1, len(returnedProviders))
		require.Equal(t, -1, tier, "tier should always be -1 with weighted selection")
		results[returnedProviders[0]]++
		tierResults[tier]++
	}
	return results, tierResults
}

// TestProviderOptimizerBasicRelayData tests the basic provider optimizer operation
// when it is updated with regular relays. Providers with bad scores should have a worse chance
// to be picked (and vice versa).
// Scenario:
//  0. There are 10 providers, the optimizer is configured to pick a single provider
//  1. Choose between 10 identical providers -> none should be in the worst tier
//  2. Append bad relay data for providers 5-7 and pick providers -> should not be 6-8
//  3. Append good relay data for providers 0-2 and pick providers -> should often be 0-2
func TestProviderOptimizerBasicRelayData(t *testing.T) {
	providerOptimizer := setupProviderOptimizer(1)
	providersGen := (&providersGenerator{}).setupProvidersForTest(10)
	rand.InitRandomSeed()
	cu := uint64(1)
	requestBlock := int64(1000)
	syncBlock := uint64(requestBlock)
	// AutoAdjustTiers = true
	// damage providers 5-7 scores with bad latency relays
	// they should not be selected by the optimizer and should be in the worst tier
	badLatency := TEST_BASE_WORLD_LATENCY * 3
	providerOptimizer.AppendRelayData(providersGen.providersAddresses[5], badLatency, cu, syncBlock)
	providerOptimizer.AppendRelayData(providersGen.providersAddresses[6], badLatency, cu, syncBlock)
	providerOptimizer.AppendRelayData(providersGen.providersAddresses[7], badLatency, cu, syncBlock)

	// improve providers 0-2 scores with good latency probes relays
	// they should be selected by the optimizer more often and should be in the best tier
	goodLatency := TEST_BASE_WORLD_LATENCY / 3
	averageLatency := TEST_BASE_WORLD_LATENCY / 2
	// add good latency relays for providers 0-2
	providerOptimizer.AppendRelayData(providersGen.providersAddresses[0], goodLatency, cu, syncBlock)
	providerOptimizer.AppendRelayData(providersGen.providersAddresses[1], goodLatency, cu, syncBlock)
	providerOptimizer.AppendRelayData(providersGen.providersAddresses[2], goodLatency, cu, syncBlock)
	// add average latency relays for providers 3,4,8,9
	providerOptimizer.AppendRelayData(providersGen.providersAddresses[3], averageLatency, cu, syncBlock)
	providerOptimizer.AppendRelayData(providersGen.providersAddresses[4], averageLatency, cu, syncBlock)
	providerOptimizer.AppendRelayData(providersGen.providersAddresses[8], averageLatency, cu, syncBlock)
	providerOptimizer.AppendRelayData(providersGen.providersAddresses[9], averageLatency, cu, syncBlock)
	// add bad latency relays for providers 5-7
	providerOptimizer.AppendRelayData(providersGen.providersAddresses[5], badLatency, cu, syncBlock)
	providerOptimizer.AppendRelayData(providersGen.providersAddresses[6], badLatency, cu, syncBlock)
	providerOptimizer.AppendRelayData(providersGen.providersAddresses[7], badLatency, cu, syncBlock)
	time.Sleep(4 * time.Millisecond)

	// Weighted selection should favor providers with better latency
	results, _ := runChooseManyTimesAndReturnResults(t, providerOptimizer, providersGen.providersAddresses, nil, 1000, cu, requestBlock)

	// With weighted selection, good latency providers (0-2) should be selected more than bad ones (5-7)
	// but distribution is more even due to minimum selection chance and other QoS factors
	goodProviderSelections := results[providersGen.providersAddresses[0]] +
		results[providersGen.providersAddresses[1]] +
		results[providersGen.providersAddresses[2]]
	badProviderSelections := results[providersGen.providersAddresses[5]] +
		results[providersGen.providersAddresses[6]] +
		results[providersGen.providersAddresses[7]]

	// Good providers should collectively get more selections than bad providers
	// (not 3x more since latency is only 30% of weight and min selection chance applies)
	require.Greater(t, goodProviderSelections, badProviderSelections,
		"good latency providers should be selected more than bad latency providers")
}

// Removed: TestProviderOptimizerBasicRelayDataAutoAdjustTiers
// This test was specific to tier-based selection with AutoAdjustTiers feature
// Tier-based selection has been replaced with weighted selection

// TestProviderOptimizerAvailabilityProbeData tests the availability update when
// the optimizer is updated with failed probe relays. Providers with bad scores should have
// a worse chance to be picked (and vice versa).
// Scenario:
//  0. There are 100 providers, the optimizer is configured to pick a single provider
//  1. Append bad probe relay data for all provider but random three
//  2. Pick providers and check they're picked most often
func TestProviderOptimizerAvailabilityProbeData(t *testing.T) {
	providerOptimizer := setupProviderOptimizer(1)
	providersCount := 100
	cu := uint64(1)
	requestBlock := int64(1000)
	providersGen := (&providersGenerator{}).setupProvidersForTest(providersCount)
	rand.InitRandomSeed()

	// damage all the providers scores with failed probe relays but three random ones
	skipIndex := rand.Intn(providersCount - 3)
	// Removed: providerOptimizer.OptimizerNumTiers = 33 (tiers removed, using weighted selection)
	for i := range providersGen.providersAddresses {
		// give all providers a worse availability score except these 3
		if i == skipIndex || i == skipIndex+1 || i == skipIndex+2 {
			// skip 0
			continue
		}
		providerOptimizer.AppendProbeRelayData(providersGen.providersAddresses[i], TEST_BASE_WORLD_LATENCY, false)
	}

	// pick providers, the three random ones with good availability should be picked more often
	time.Sleep(4 * time.Millisecond)
	results, _ := runChooseManyTimesAndReturnResults(t, providerOptimizer, providersGen.providersAddresses, nil, 1000, cu, requestBlock)
	// With weighted selection and minimum selection chance, good providers get more but not overwhelming majority
	// Each of the 3 good providers should get more than average (1000/100 = 10 per provider average)
	averageSelections := 1000 / len(providersGen.providersAddresses)
	require.Greater(t, results[providersGen.providersAddresses[skipIndex]], averageSelections,
		"good availability provider should be selected more than average")
	require.Greater(t, results[providersGen.providersAddresses[skipIndex+1]], averageSelections,
		"good availability provider should be selected more than average")
	require.Greater(t, results[providersGen.providersAddresses[skipIndex+2]], averageSelections,
		"good availability provider should be selected more than average")
	require.InDelta(t, results[providersGen.providersAddresses[skipIndex]], results[providersGen.providersAddresses[skipIndex+1]], 50)

	// pick providers again but this time ignore one of the random providers, it shouldn't be picked
	results, _ = runChooseManyTimesAndReturnResults(t, providerOptimizer, providersGen.providersAddresses, map[string]struct{}{providersGen.providersAddresses[skipIndex]: {}}, 1000, cu, requestBlock)
	require.Zero(t, results[providersGen.providersAddresses[skipIndex]])
}

// TestProviderOptimizerAvailabilityProbeData tests the availability update when
// the optimizer is updated with failed relays. Providers with bad scores should have
// a worse chance to be picked (and vice versa).
// Scenario:
//  0. There are 100 providers, the optimizer is configured to pick a single provider
//  1. Append bad probe relay data for all provider but random three
//  2. Pick providers and check they're picked most often
func TestProviderOptimizerAvailabilityRelayData(t *testing.T) {
	providerOptimizer := setupProviderOptimizer(1)
	providersCount := 100
	cu := uint64(10)
	requestBlock := int64(1000)
	providersGen := (&providersGenerator{}).setupProvidersForTest(providersCount)
	rand.InitRandomSeed()

	// damage all the providers scores with failed probe relays but three random ones
	skipIndex := rand.Intn(providersCount - 3)
	// Removed: providerOptimizer.OptimizerNumTiers = 33 (tiers removed, using weighted selection)
	for i := range providersGen.providersAddresses {
		// give all providers a worse availability score except these 3
		if i == skipIndex || i == skipIndex+1 || i == skipIndex+2 {
			// skip 0
			continue
		}
		providerOptimizer.AppendRelayFailure(providersGen.providersAddresses[i])
	}

	// pick providers, the three random ones with good availability should be picked more often
	time.Sleep(4 * time.Millisecond)
	results, _ := runChooseManyTimesAndReturnResults(t, providerOptimizer, providersGen.providersAddresses, nil, 1000, cu, requestBlock)
	// With weighted selection, good availability providers should get more than average
	averageSelections := 1000 / len(providersGen.providersAddresses)
	require.Greater(t, results[providersGen.providersAddresses[skipIndex]], averageSelections,
		"good availability provider should be selected more than average")
	require.Greater(t, results[providersGen.providersAddresses[skipIndex+1]], averageSelections,
		"good availability provider should be selected more than average")
	require.Greater(t, results[providersGen.providersAddresses[skipIndex+2]], averageSelections,
		"good availability provider should be selected more than average")
	require.InDelta(t, results[providersGen.providersAddresses[skipIndex]], results[providersGen.providersAddresses[skipIndex+1]], float64(averageSelections)*2)

	// pick providers again but this time ignore one of the random providers, it shouldn't be picked
	results, _ = runChooseManyTimesAndReturnResults(t, providerOptimizer, providersGen.providersAddresses, map[string]struct{}{providersGen.providersAddresses[skipIndex]: {}}, 1000, cu, requestBlock)
	require.Zero(t, results[providersGen.providersAddresses[skipIndex]])
}

func TestProviderOptimizerAvailabilityBlockError(t *testing.T) {
	providerOptimizer := setupProviderOptimizer(1)
	providersCount := 10
	providersGen := (&providersGenerator{}).setupProvidersForTest(providersCount)
	rand.InitRandomSeed()
	cu := uint64(10)
	requestBlock := int64(1000)
	syncBlock := uint64(1000)
	badSyncBlock := syncBlock - 1

	// damage all the providers scores with bad sync relays but three random ones
	// the three random providers also get slightly worse latency
	// bad sync means an update that doesn't have the latest requested block
	chosenIndex := rand.Intn(providersCount - 2)
	threeOthers := []int{}
	for i := range providersGen.providersAddresses {
		time.Sleep(4 * time.Millisecond)
		if i == chosenIndex || i == chosenIndex+1 || i == chosenIndex+2 {
			slightlyBadLatency := TEST_BASE_WORLD_LATENCY + 100*time.Millisecond
			providerOptimizer.AppendRelayData(providersGen.providersAddresses[i], slightlyBadLatency, cu, syncBlock)
			continue
		}
		if len(threeOthers) < 3 {
			threeOthers = append(threeOthers, i)
		}
		providerOptimizer.AppendRelayData(providersGen.providersAddresses[i], TEST_BASE_WORLD_LATENCY, cu, badSyncBlock)
	}

	time.Sleep(4 * time.Millisecond)

	// Weighted selection should favor providers with lower block error probability
	results, _ := runChooseManyTimesAndReturnResults(t, providerOptimizer, providersGen.providersAddresses, nil, 1000, cu, requestBlock)

	// With weighted selection, providers with better sync should get more selections
	// But distribution is more even across all providers due to other QoS factors and minimum selection chance
	sumGoodSync := results[providersGen.providersAddresses[chosenIndex]] + results[providersGen.providersAddresses[chosenIndex+1]] + results[providersGen.providersAddresses[chosenIndex+2]]
	sumBadSync := 0
	for i, addr := range providersGen.providersAddresses {
		if i != chosenIndex && i != chosenIndex+1 && i != chosenIndex+2 {
			sumBadSync += results[addr]
		}
	}
	// Good sync providers should collectively get more than bad sync providers (3 vs 7 providers)
	// Normalized by provider count: sumGoodSync/3 should be > sumBadSync/7
	avgGoodSync := float64(sumGoodSync) / 3.0
	avgBadSync := float64(sumBadSync) / 7.0
	require.Greater(t, avgGoodSync, avgBadSync, "providers with good sync should be selected more on average")
}

// TestProviderOptimizerUpdatingLatency tests checks that repeatedly adding better results
// (with both probes and relays) makes the latency score improve
func TestProviderOptimizerUpdatingLatency(t *testing.T) {
	providerOptimizer := setupProviderOptimizer(1)
	providersCount := 2
	providersGen := (&providersGenerator{}).setupProvidersForTest(providersCount)
	providerAddress := providersGen.providersAddresses[0]
	cu := uint64(10)
	requestBlock := int64(1000)
	syncBlock := uint64(requestBlock)

	// add an average latency probe relay to determine average score
	providerOptimizer.AppendProbeRelayData(providerAddress, TEST_BASE_WORLD_LATENCY, true)
	time.Sleep(4 * time.Millisecond)

	// add good latency probe relays, score should improve
	for i := 0; i < 10; i++ {
		// get current score
		qos, _ := providerOptimizer.GetReputationReportForProvider(providerAddress)
		require.NotNil(t, qos)
		score, err := qos.ComputeReputation()
		require.NoError(t, err)

		// add good latency probe
		providerOptimizer.AppendProbeRelayData(providerAddress, TEST_BASE_WORLD_LATENCY/10, true)
		time.Sleep(4 * time.Millisecond)

		// check score again and compare to the last score
		qos, _ = providerOptimizer.GetReputationReportForProvider(providerAddress)
		require.NotNil(t, qos)
		newScore, err := qos.ComputeReputation()
		require.NoError(t, err)
		require.True(t, newScore.LT(score), "newScore: "+newScore.String()+", score: "+score.String())
	}

	// add an average latency probe relay to determine average score
	providerAddress = providersGen.providersAddresses[1]
	providerOptimizer.AppendRelayData(providerAddress, TEST_BASE_WORLD_LATENCY, cu, syncBlock)
	time.Sleep(4 * time.Millisecond)

	// add good latency relays, score should improve
	for i := 0; i < 10; i++ {
		// get current score
		qos, _ := providerOptimizer.GetReputationReportForProvider(providerAddress)
		require.NotNil(t, qos)
		score, err := qos.ComputeReputation()
		require.NoError(t, err)

		// add good latency relay
		providerOptimizer.AppendRelayData(providerAddress, TEST_BASE_WORLD_LATENCY/10, cu, syncBlock)
		time.Sleep(4 * time.Millisecond)

		// check score again and compare to the last score
		qos, _ = providerOptimizer.GetReputationReportForProvider(providerAddress)
		require.NotNil(t, qos)
		newScore, err := qos.ComputeReputation()
		require.NoError(t, err)
		require.True(t, newScore.LT(score), "newScore: "+newScore.String()+", score: "+score.String())
	}
}

func TestProviderOptimizerExploration(t *testing.T) {
	providerOptimizer := setupProviderOptimizer(2)
	providersCount := 10
	providersGen := (&providersGenerator{}).setupProvidersForTest(providersCount)
	cu := uint64(10)
	requestBlock := int64(1000)
	syncBlock := uint64(requestBlock)

	rand.InitRandomSeed()
	// start with a disabled chosen index
	chosenIndex := -1
	testProvidersExploration := func(iterations int) float64 {
		exploration := 0.0
		for i := 0; i < iterations; i++ {
			returnedProviders, _ := providerOptimizer.ChooseProvider(providersGen.providersAddresses, nil, cu, requestBlock)
			if len(returnedProviders) > 1 {
				exploration++
				// check if we have a specific chosen index
				if chosenIndex >= 0 {
					// there's only one provider eligible for exploration it must be him
					require.Equal(t, providersGen.providersAddresses[chosenIndex], returnedProviders[1])
				}
			}
		}
		return exploration
	}

	// make sure exploration works when providers are defaulted (no data at all)
	exploration := testProvidersExploration(1000)
	require.Greater(t, exploration, float64(10))

	chosenIndex = rand.Intn(providersCount - 2)
	// set chosen index with a value in the past so it can be selected for exploration
	providerOptimizer.appendRelayData(providersGen.providersAddresses[chosenIndex], TEST_BASE_WORLD_LATENCY*2, true, cu, syncBlock, time.Now().Add(-35*time.Second))
	// set a basic state for all other provider, with a recent time (so they can't be selected for exploration)
	for i := 0; i < 10; i++ {
		for index, address := range providersGen.providersAddresses {
			if index == chosenIndex {
				// we set chosenIndex with a past time so it can be selected for exploration
				continue
			}
			// set samples in the future so they are never a candidate for exploration
			providerOptimizer.appendRelayData(address, TEST_BASE_WORLD_LATENCY*2, true, cu, syncBlock, time.Now().Add(1*time.Second))
		}
		time.Sleep(4 * time.Millisecond)
	}

	// with a cost strategy we expect exploration at a 10% rate
	providerOptimizer.strategy = StrategyBalanced         // that's the default but to be explicit
	providerOptimizer.wantedNumProvidersInConcurrency = 2 // that's in the constructor but to be explicit
	iterations := 10000
	exploration = testProvidersExploration(iterations)
	require.Less(t, exploration, float64(1.4)*float64(iterations)*DefaultExplorationChance)    // allow mistake buffer of 40% because of randomness
	require.Greater(t, exploration, float64(0.6)*float64(iterations)*DefaultExplorationChance) // allow mistake buffer of 40% because of randomness

	// with a cost strategy we expect exploration to happen once in 100 samples
	providerOptimizer.strategy = StrategyCost
	exploration = testProvidersExploration(iterations)
	require.Less(t, exploration, float64(1.4)*float64(iterations)*CostExplorationChance)    // allow mistake buffer of 40% because of randomness
	require.Greater(t, exploration, float64(0.6)*float64(iterations)*CostExplorationChance) // allow mistake buffer of 40% because of randomness

	// privacy disables exploration
	providerOptimizer.strategy = StrategyPrivacy
	exploration = testProvidersExploration(iterations)
	require.Equal(t, exploration, float64(0))
}

func TestProviderOptimizerSyncScore(t *testing.T) {
	providerOptimizer := setupProviderOptimizer(1)
	providersGen := (&providersGenerator{}).setupProvidersForTest(10)
	rand.InitRandomSeed()
	cu := uint64(10)
	requestBlock := spectypes.LATEST_BLOCK

	syncBlock := uint64(1000)

	chosenIndex := rand.Intn(len(providersGen.providersAddresses))
	sampleTime := time.Now()
	for j := 0; j < 3; j++ { // repeat several times because a sync score is only correct after all providers sent their first block otherwise its giving favor to the first one
		for i := range providersGen.providersAddresses {
			time.Sleep(4 * time.Millisecond)
			if i == chosenIndex {
				// give better syncBlock, latency is a tiny bit worse for the second check
				providerOptimizer.appendRelayData(providersGen.providersAddresses[i], TEST_BASE_WORLD_LATENCY*2+1*time.Microsecond, true, cu, syncBlock+5, sampleTime)
				continue
			}
			providerOptimizer.appendRelayData(providersGen.providersAddresses[i], TEST_BASE_WORLD_LATENCY*2, true, cu, syncBlock, sampleTime) // update that he doesn't have the latest requested block
		}
		sampleTime = sampleTime.Add(time.Millisecond * 5)
	}
	time.Sleep(4 * time.Millisecond)

	// Weighted selection should favor the provider with better sync (chosenIndex has syncBlock+5)
	results, _ := runChooseManyTimesAndReturnResults(t, providerOptimizer, providersGen.providersAddresses, nil, 1000, cu, requestBlock)
	// Provider with better sync should be selected more than average
	averageSelections := 1000 / len(providersGen.providersAddresses)
	require.Greater(t, results[providersGen.providersAddresses[chosenIndex]], averageSelections,
		"provider with better sync should be selected more than average")
}

// Removed: TestProviderOptimizerStrategiesScoring
// This test was heavily dependent on tier-based selection verification
// Strategy-specific behavior is now tested through weighted selector tests

func TestReputation(t *testing.T) {
	providerOptimizer := setupProviderOptimizer(1)
	providersCount := 5
	providersGen := (&providersGenerator{}).setupProvidersForTest(providersCount)
	cu := uint64(10)
	syncBlock := uint64(1000)
	// set a basic state for all of them
	sampleTime := time.Now()
	for i := 0; i < 10; i++ {
		for _, address := range providersGen.providersAddresses {
			providerOptimizer.appendRelayData(address, TEST_BASE_WORLD_LATENCY*2, true, cu, syncBlock, sampleTime)
		}
		time.Sleep(4 * time.Millisecond)
	}
	report, sampleTime1 := providerOptimizer.GetReputationReportForProvider(providersGen.providersAddresses[0])
	require.NotNil(t, report)
	require.True(t, sampleTime.Equal(sampleTime1))
	report2, sampleTime2 := providerOptimizer.GetReputationReportForProvider(providersGen.providersAddresses[1])
	require.NotNil(t, report2)
	require.Equal(t, report, report2)
	require.True(t, sampleTime.Equal(sampleTime2))
}

// test low providers count 0-9
func TestProviderOptimizerProvidersCount(t *testing.T) {
	rand.InitRandomSeed()
	providerOptimizer := setupProviderOptimizer(1)
	providersCount := 10
	providersGen := (&providersGenerator{}).setupProvidersForTest(providersCount)
	cu := uint64(10)
	requestBlock := int64(1000)
	syncBlock := uint64(1000)
	sampleTime := time.Now()
	for i := 0; i < 10; i++ {
		for _, address := range providersGen.providersAddresses {
			providerOptimizer.appendRelayData(address, TEST_BASE_WORLD_LATENCY*2, true, cu, syncBlock, sampleTime)
		}
		time.Sleep(4 * time.Millisecond)
	}
	playbook := []struct {
		name      string
		providers int
	}{
		{name: "one", providers: 1},
		{name: "two", providers: 2},
		{name: "three", providers: 3},
		{name: "four", providers: 4},
		{name: "five", providers: 5},
		{name: "six", providers: 6},
		{name: "seven", providers: 7},
		{name: "eight", providers: 8},
		{name: "nine", providers: 9},
	}
	for _, play := range playbook {
		t.Run(play.name, func(t *testing.T) {
			for i := 0; i < 10; i++ {
				returnedProviders, _ := providerOptimizer.ChooseProvider(providersGen.providersAddresses[:play.providers], nil, cu, requestBlock)
				require.Greater(t, len(returnedProviders), 0)
			}
		})
	}
}

func TestProviderOptimizerWeights(t *testing.T) {
	rand.InitRandomSeed()
	providerOptimizer := setupProviderOptimizer(1)
	providersCount := 10
	providersGen := (&providersGenerator{}).setupProvidersForTest(providersCount)
	cu := uint64(10)
	requestBlock := spectypes.LATEST_BLOCK
	syncBlock := uint64(1000)
	weights := map[string]int64{
		providersGen.providersAddresses[0]: 10000000000000, // simulating 10m tokens
	}
	for i := 1; i < 10; i++ {
		weights[providersGen.providersAddresses[i]] = 50000000000
	}

	normalLatency := TEST_BASE_WORLD_LATENCY * 2
	improvedLatency := normalLatency - 5*time.Millisecond
	improvedBlock := syncBlock + 10

	providerOptimizer.UpdateWeights(weights, 1)
	for i := 0; i < 10; i++ {
		for idx, address := range providersGen.providersAddresses {
			if idx == 0 {
				providerOptimizer.appendRelayData(address, normalLatency, true, cu, improvedBlock, time.Now())
			} else {
				providerOptimizer.appendRelayData(address, improvedLatency, true, cu, syncBlock, time.Now())
			}
			time.Sleep(4 * time.Millisecond)
		}
	}

	// Weighted selection should favor provider 0 with better stake
	results, _ := runChooseManyTimesAndReturnResults(t, providerOptimizer, providersGen.providersAddresses, nil, 1000, cu, requestBlock)

	// Provider 0 should be selected most often due to high stake weight
	maxCount := results[providersGen.providersAddresses[0]]
	for addr, count := range results {
		if addr != providersGen.providersAddresses[0] {
			require.GreaterOrEqual(t, maxCount, count, "provider 0 should have highest selection count")
		}
	}
}

// Removed: TestProviderOptimizerTiers
// This test was specific to tier-based selection system which has been replaced with weighted selection

// TestProviderOptimizerChooseProvider checks that the follwing occurs:
// 0. Assume 6 providers: 2 with great score, 2 with mid score but one has a great stake, and 2 with low score (benchmark).
// We choose 2 providers in each choice. We choose many times.
// 1. ~80% of the times, the great score providers are picked (no preference between the two)
// 2. high stake mid score is picked more than 0 times and picked more than mid score with average stake
// 3. low score are not selected
func TestProviderOptimizerChooseProvider(t *testing.T) {
	rand.InitRandomSeed()
	providerOptimizer := setupProviderOptimizer(1)
	providersCount := 6
	// Removed: providerOptimizer.OptimizerNumTiers = providersCount / 2 (tiers removed)
	// Removed: providerOptimizer.OptimizerMinTierEntries = 2 (tiers removed)
	providersGen := (&providersGenerator{}).setupProvidersForTest(providersCount)
	cu := uint64(10)
	requestBlock := spectypes.LATEST_BLOCK
	syncBlock := uint64(1000)
	sampleTime := time.Now()

	// apply high stake for provider 2
	normalStake := int64(50000000000)
	highStake := 5 * normalStake
	highStakeProviderIndex := 2
	weights := map[string]int64{}
	for i := 0; i < providersCount; i++ {
		if i == highStakeProviderIndex {
			weights[providersGen.providersAddresses[i]] = highStake
		} else {
			weights[providersGen.providersAddresses[i]] = normalStake
		}
	}
	providerOptimizer.UpdateWeights(weights, 1)

	// setup scores to all providers
	improvedLatency := TEST_BASE_WORLD_LATENCY / 2
	normalLatency := TEST_BASE_WORLD_LATENCY * 2
	improvedBlock := syncBlock + 1

	// provider 0 and 1 gets a good latency and good sync
	providerOptimizer.appendRelayData(providersGen.providersAddresses[0], improvedLatency, true, cu, improvedBlock, sampleTime)
	providerOptimizer.appendRelayData(providersGen.providersAddresses[1], improvedLatency, true, cu, improvedBlock, sampleTime)

	// providers 2 and 3 get a good latency only
	providerOptimizer.appendRelayData(providersGen.providersAddresses[2], improvedLatency, true, cu, syncBlock, sampleTime)
	providerOptimizer.appendRelayData(providersGen.providersAddresses[3], improvedLatency, true, cu, syncBlock, sampleTime)

	// provider 4 and 5 gets a normal latency and sync
	providerOptimizer.appendRelayData(providersGen.providersAddresses[4], normalLatency, true, cu, syncBlock, sampleTime)
	providerOptimizer.appendRelayData(providersGen.providersAddresses[5], normalLatency, true, cu, syncBlock, sampleTime)

	// now repeat to modify all providers scores across sync calculation
	sampleTime = sampleTime.Add(10 * time.Millisecond)
	time.Sleep(10 * time.Millisecond)
	providerOptimizer.appendRelayData(providersGen.providersAddresses[5], normalLatency, true, cu, syncBlock, sampleTime)
	providerOptimizer.appendRelayData(providersGen.providersAddresses[4], normalLatency, true, cu, syncBlock, sampleTime)
	providerOptimizer.appendRelayData(providersGen.providersAddresses[3], improvedLatency, true, cu, syncBlock, sampleTime)
	providerOptimizer.appendRelayData(providersGen.providersAddresses[2], improvedLatency, true, cu, syncBlock, sampleTime)
	providerOptimizer.appendRelayData(providersGen.providersAddresses[1], improvedLatency, true, cu, improvedBlock, sampleTime)
	providerOptimizer.appendRelayData(providersGen.providersAddresses[0], improvedLatency, true, cu, improvedBlock, sampleTime)
	time.Sleep(4 * time.Millisecond)

	// choose many times and check results
	iterations := 10000
	results, _ := runChooseManyTimesAndReturnResults(t, providerOptimizer, providersGen.providersAddresses, nil, iterations, cu, requestBlock)
	// High score providers (0 and 1) should get more selections than low score providers
	// With 6 providers and weighted selection, distribution is more balanced than with tiers
	highScoreSelections := results[providersGen.providersAddresses[0]] + results[providersGen.providersAddresses[1]]
	lowScoreSelections := results[providersGen.providersAddresses[4]] + results[providersGen.providersAddresses[5]]
	require.Greater(t, highScoreSelections, lowScoreSelections, "high score providers should get more selections than low score")
	require.InDelta(t, results[providersGen.providersAddresses[0]],
		results[providersGen.providersAddresses[1]], float64(results[providersGen.providersAddresses[0]])*0.15) // no difference between high score providers (max 10% diff)
	require.Greater(t, results[providersGen.providersAddresses[2]], 0)                                           // high stake mid score provider picked at least once
	require.Greater(t, results[providersGen.providersAddresses[2]], results[providersGen.providersAddresses[3]]) // high stake mid score provider picked more than normal stake mid score provider
	require.Less(t, results[providersGen.providersAddresses[4]], results[providersGen.providersAddresses[2]])
	require.Less(t, results[providersGen.providersAddresses[5]], results[providersGen.providersAddresses[2]])
}

// TestProviderOptimizerRetriesWithReducedProvidersSet checks that when having a set of providers, the amount of
// providers doesn't matter and the choice is deterministic. The test does the following:
// 0. Assume a set of providers (great/mid/low score with high/low stake, all combinations)
// 1. Run ChooseProvider() <providers_amount> number of times. Each iteration, the chosen provider from the
// last iteration is removed from the providers set. We check the ranking of providers stays the same.
// 2. Do step 1 many times.
// Expected: the ranking of providers stays the same, providers with high stake are picked more often,
// providers from the lowest tier are not picked
func TestProviderOptimizerRetriesWithReducedProvidersSet(t *testing.T) {
	rand.InitRandomSeed()
	providerOptimizer := setupProviderOptimizer(1)
	providersCount := 6
	providersGen := (&providersGenerator{}).setupProvidersForTest(providersCount)

	// create 3 tiers with 2 providers each
	// Removed: providerOptimizer.OptimizerNumTiers = providersCount / 2 (tiers removed)
	// Removed: providerOptimizer.OptimizerMinTierEntries = 2 (tiers removed)

	// apply high stake for providers 1, 3, 5
	normalStake := int64(50000000000)
	highStake := 5 * normalStake
	highStakeProviderIndexes := []int{1, 3, 5}
	weights := map[string]int64{}
	for i := 0; i < providersCount; i++ {
		if lavaslices.Contains(highStakeProviderIndexes, i) {
			weights[providersGen.providersAddresses[i]] = highStake
		} else {
			weights[providersGen.providersAddresses[i]] = normalStake
		}
	}
	providerOptimizer.UpdateWeights(weights, 1)
	cu := uint64(10)
	requestBlock := int64(1000)
	syncBlock := uint64(1000)
	sampleTime := time.Now()
	baseLatency := TEST_BASE_WORLD_LATENCY.Seconds()

	// append relay data for each provider depending on its index in the providers array
	// the latency gets worse for increasing index so we assume the best provider is the 1st
	// address, after it the 2nd and so on
	for i := 0; i < 50; i++ {
		for j, address := range providersGen.providersAddresses {
			latency := time.Duration(baseLatency * float64(2*j+1) * float64(time.Millisecond))
			providerOptimizer.appendRelayData(address, latency, true, cu, syncBlock, sampleTime)
		}
		sampleTime = sampleTime.Add(5 * time.Millisecond)
		time.Sleep(5 * time.Millisecond)
	}

	// With weighted selection, providers with better latency should be selected more often
	// Stake weight is only 10% so QoS dominates selection
	iterations := 1000
	results, _ := runChooseManyTimesAndReturnResults(t, providerOptimizer, providersGen.providersAddresses, nil, iterations, cu, requestBlock)

	// Providers with better latency should be selected more than worse latency providers
	// Provider 0 has best latency (index * 2 + 1 milliseconds formula), should be selected most
	require.Greater(t, results[providersGen.providersAddresses[0]], results[providersGen.providersAddresses[5]],
		"best latency provider (0) should be selected more than worst latency provider (5)")

	// Among similar latency providers, higher stake should give advantage (but stake is only 10% weight)
	// Providers 0 (low stake) vs 1 (high stake) have similar latency, 1 should be selected more
	// However, with only 10% stake weight and randomness, this might be close
	totalSelectionsTopTwo := results[providersGen.providersAddresses[0]] + results[providersGen.providersAddresses[1]]
	require.Greater(t, totalSelectionsTopTwo, iterations/5,
		"top two providers should collectively get >20% of selections")
}

// TestProviderOptimizerChoiceSimulationBasedOnLatency checks that the overall choice mechanism acts as expected,
// For each of the following metrics: latency, sync, availability and stake we do the following:
// 0. Assume 3 providers
// 1. Append relay data for both providers with random samples. The "better" provider will have a randomized
// sample with a better range (for example, the better one gets latency of 10-30ms and the bad one gets 25-40ms)
// 2. Choose between them and verify the better one is chosen more.
func TestProviderOptimizerChoiceSimulationBasedOnLatency(t *testing.T) {
	rand.InitRandomSeed()
	providerOptimizer := setupProviderOptimizer(1)
	providersCount := 3
	providersGen := (&providersGenerator{}).setupProvidersForTest(providersCount)
	cu := uint64(10)
	requestBlock := int64(1000)
	syncBlock := uint64(1000)
	baseLatency := TEST_BASE_WORLD_LATENCY.Seconds()
	// Removed: providerOptimizer.OptimizerNumTiers = 4 (tiers removed)
	// Removed: providerOptimizer.OptimizerMinTierEntries = 1 (tiers removed)

	// initial values
	p1Latency := baseLatency * float64(time.Millisecond)
	p2Latency := baseLatency * float64(time.Millisecond)
	p3Latency := baseLatency * float64(time.Millisecond)
	p1SyncBlock := syncBlock
	p2SyncBlock := syncBlock
	p3SyncBlock := syncBlock
	p1Availability := true
	p2Availability := true
	p3Availability := true
	// append relay data for each provider depending on its index in the providers array
	// the latency gets worse for increasing index so we assume the best provider is the 1st
	// address, after it the 2nd and so on
	for i := 0; i < 1000; i++ {
		// randomize latency, provider 0 gets a better latency than provider 1
		p1Latency += 10 * float64(time.Millisecond)
		p2Latency += 20 * float64(time.Millisecond)
		p3Latency += 30 * float64(time.Millisecond)

		// randomize sync, provider 0 gets a better sync than provider 1
		p1SyncBlock++
		p2SyncBlock++
		p3SyncBlock++

		time.Sleep(1 * time.Millisecond)
		providerOptimizer.appendRelayData(providersGen.providersAddresses[0], time.Duration(p1Latency), p1Availability, cu, p1SyncBlock, time.Now())
		providerOptimizer.appendRelayData(providersGen.providersAddresses[1], time.Duration(p2Latency), p2Availability, cu, p2SyncBlock, time.Now())
		providerOptimizer.appendRelayData(providersGen.providersAddresses[2], time.Duration(p3Latency), p3Availability, cu, p3SyncBlock, time.Now())
	}

	// choose many times and check distribution
	// With weighted selection, latency weight is 30%, so differences should be noticeable but not extreme
	iterations := 1000
	res, _ := runChooseManyTimesAndReturnResults(t, providerOptimizer, providersGen.providersAddresses, nil, iterations, cu, requestBlock)
	utils.LavaFormatInfo("res", utils.LogAttr("res", res))

	// With weighted selection and equal availability, latency differences should result in
	// provider 0 (best latency) getting more selections than provider 2 (worst latency)
	// But due to randomness and other factors, the ordering might not be strict
	require.Greater(t, res[providersGen.providersAddresses[0]], res[providersGen.providersAddresses[2]],
		"best latency provider should be selected more than worst latency provider")

	// All providers should get some selections (minimum selection chance ensures this)
	for i := 0; i < 3; i++ {
		require.Greater(t, res[providersGen.providersAddresses[i]], 0, "each provider should be selected at least once")
	}
}

func TestProviderOptimizerChoiceSimulationBasedOnSync(t *testing.T) {
	rand.InitRandomSeed()
	providerOptimizer := setupProviderOptimizer(1)
	providersCount := 3
	providersGen := (&providersGenerator{}).setupProvidersForTest(providersCount)
	cu := uint64(10)
	syncBlock := uint64(1000)
	baseLatency := TEST_BASE_WORLD_LATENCY.Seconds()
	// Removed: providerOptimizer.OptimizerNumTiers = 4 (tiers removed)
	// Removed: providerOptimizer.OptimizerMinTierEntries = 1 (tiers removed)

	// initial values
	p1Latency := baseLatency * float64(time.Millisecond)
	p2Latency := baseLatency * float64(time.Millisecond)
	p3Latency := baseLatency * float64(time.Millisecond)
	p1SyncBlock := syncBlock
	p2SyncBlock := syncBlock
	p3SyncBlock := syncBlock
	p1Availability := true
	p2Availability := true
	p3Availability := true
	// append relay data for each provider depending on its index in the providers array
	// the latency gets worse for increasing index so we assume the best provider is the 1st
	// address, after it the 2nd and so on
	sampleTime := time.Now()
	for i := 0; i < 1000; i++ {
		// randomize latency, provider 0 gets a better latency than provider 1
		p1Latency += 10 * float64(time.Millisecond)
		p2Latency += 10 * float64(time.Millisecond)
		p3Latency += 10 * float64(time.Millisecond)

		// randomize sync, provider 0 gets a better sync than provider 1
		p1SyncBlock++
		if i%100 == 1 {
			p1SyncBlock++
		}
		if i%300 == 1 {
			p2SyncBlock++
		}
		p2SyncBlock++
		p3SyncBlock++

		time.Sleep(1 * time.Millisecond)
		providerOptimizer.appendRelayData(providersGen.providersAddresses[0], time.Duration(p1Latency), p1Availability, cu, p1SyncBlock, sampleTime)
		providerOptimizer.appendRelayData(providersGen.providersAddresses[1], time.Duration(p2Latency), p2Availability, cu, p2SyncBlock, sampleTime)
		providerOptimizer.appendRelayData(providersGen.providersAddresses[2], time.Duration(p3Latency), p3Availability, cu, p3SyncBlock, sampleTime)
	}
	// choose many times and check the better provider is chosen more often (provider 0)
	iterations := 1000
	res, tierResults := runChooseManyTimesAndReturnResults(t, providerOptimizer, providersGen.providersAddresses, nil, iterations, cu, int64(p1SyncBlock))

	utils.LavaFormatInfo("res", utils.LogAttr("res", res), utils.LogAttr("tierResults", tierResults))
	require.Greater(t, res[providersGen.providersAddresses[0]], res[providersGen.providersAddresses[1]])
	require.Greater(t, res[providersGen.providersAddresses[0]], res[providersGen.providersAddresses[2]])
	require.Greater(t, res[providersGen.providersAddresses[1]], res[providersGen.providersAddresses[2]])
}

// TestProviderOptimizerLatencySyncScore tests that a provider with 100ms latency and x sync block
// has the same score as a provider with 1100ms latency but x+1 sync block
// This is true since the average block time is 10sec and the default sync factor is 0.3. So
// score_good_latency = latency + sync_factor * sync_lag + ... = 0.01 + 0.3 * 10 + ... = 3.01 + ...
// score_good_sync = latency + sync_factor * sync_lag + ... = 3.01 + 0.3 * 0 + ... = 3.01 + ...
func TestProviderOptimizerLatencySyncScore(t *testing.T) {
	rand.InitRandomSeed()
	providerOptimizer := setupProviderOptimizer(1)
	providersCount := 2
	providersGen := (&providersGenerator{}).setupProvidersForTest(providersCount)
	cu := uint64(10)
	requestBlock := spectypes.LATEST_BLOCK
	syncBlock := uint64(1000)

	improvedLatency := TEST_BASE_WORLD_LATENCY
	badLatency := TEST_BASE_WORLD_LATENCY + 3*time.Second // sync factor is 0.3 so add 3 seconds

	// set a basic state for all providers
	sampleTime := time.Now()
	for i := 0; i < 10; i++ {
		for _, address := range providersGen.providersAddresses {
			providerOptimizer.appendRelayData(address, TEST_BASE_WORLD_LATENCY*2, true, cu, syncBlock, sampleTime)
		}
		time.Sleep(4 * time.Millisecond)
	}

	// provider 0 gets a good sync with bad latency
	providerOptimizer.appendRelayData(providersGen.providersAddresses[0], badLatency, true, cu, syncBlock+1, sampleTime)

	// provider 1 gets a good latency with bad sync
	providerOptimizer.appendRelayData(providersGen.providersAddresses[1], improvedLatency, true, cu, syncBlock, sampleTime.Add(TEST_AVERAGE_BLOCK_TIME))

	// verify both providers have the same score
	scores := []math.LegacyDec{}
	for _, provider := range providersGen.providersAddresses {
		qos, _ := providerOptimizer.GetReputationReportForProvider(provider)
		require.NotNil(t, qos)
		score, err := qos.ComputeReputation()
		require.NoError(t, err)
		scores = append(scores, score)
	}
	require.Len(t, scores, 2)
	s0, err := scores[0].Float64()
	require.NoError(t, err)
	s1, err := scores[1].Float64()
	require.NoError(t, err)
	require.InDelta(t, s0, s1, 0.01)

	// choose many times - since their scores should be the same, they should be picked in a similar amount
	iterations := 1000
	res, _ := runChooseManyTimesAndReturnResults(t, providerOptimizer, providersGen.providersAddresses, nil, iterations, cu, requestBlock)
	require.InDelta(t, res[providersGen.providersAddresses[0]], res[providersGen.providersAddresses[1]], float64(iterations)*0.1)
}
